# AlphaZero Pipeline Configuration

# Game settings
game_type: gomoku
board_size: 15
policy_size: 0

# Gomoku-specific settings
gomoku_use_renju: true          # Use Renju rules (restricts Black)
gomoku_use_omok: false          # Use Omok rules (typically exclusive with Renju)
gomoku_use_pro_long_opening: true  # Use pro-long opening restrictions

# Chess-specific settings (when game_type is chess)
chess_use_chess960: false       # Enable Chess960 variant

# Go-specific settings (when game_type is go)
go_komi: 7.5                   # Komi value (compensation for White)
go_chinese_rules: true         # Use Chinese rules (area scoring)
go_enforce_superko: true       # Enforce superko rule (prevent board repetition)

# Directory settings
model_dir: models
data_dir: data
log_dir: logs

# Neural network settings
network_type: resnet
use_gpu: true
num_iterations: 10
num_res_blocks: 12  # Increased for 8GB VRAM - original was 19
num_filters: 128  # Increased for 8GB VRAM - original was 256
input_channels: 17  # Match Gomoku's enhanced tensor representation (17 planes)

# Self-play settings
self_play_num_games: 400
self_play_num_parallel_games: 1
self_play_max_moves: 0
self_play_temperature_threshold: 30
self_play_high_temperature: 1.0
self_play_low_temperature: 0.1

# MCTS settings
mcts_num_simulations: 400  # Increased from 8 to achieve better play (we fixed the performance issues)
mcts_num_threads: 20  # Reduced thread count to ensure stable batching
mcts_batch_size: 64  # Larger batch size for better GPU throughput
mcts_transposition_table_size_mb: 32  # Reduce from 128MB to 32MB per engine
mcts_use_transposition_table: true
mcts_batch_timeout_ms: 50  # Balanced timeout for better batching
mcts_exploration_constant: 1.5
mcts_temperature: 1.0
mcts_virtual_loss: 1  # Reduced from default of 3 to allow more thread overlap
mcts_add_dirichlet_noise: true
mcts_dirichlet_alpha: 0.3
mcts_dirichlet_epsilon: 0.25

# Progressive widening settings
mcts_use_progressive_widening: true
mcts_progressive_widening_c: 5.0  # Expand more children initially
mcts_progressive_widening_k: 0.5  # Growth rate based on visit count

# Root parallelization settings
mcts_use_root_parallelization: true
mcts_num_root_workers: 4  # Number of parallel MCTS trees (threads will be divided among them)

# Training settings
train_epochs: 10
train_batch_size: 256
train_num_workers: 4  # Increased to maximum for better performance
train_learning_rate: 0.001
train_weight_decay: 0.0001
train_lr_step_size: 5
train_lr_gamma: 0.1

# Arena/evaluation settings
enable_evaluation: true
arena_num_games: 20
arena_num_parallel_games: 4
arena_num_threads: 4
arena_num_simulations: 100
arena_temperature: 0.1
arena_win_rate_threshold: 0.55
