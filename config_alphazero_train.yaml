# Complete AlphaZero Training Pipeline Configuration
# Optimized for Ryzen 9 5900X (24 threads) + RTX 3060 Ti (8GB VRAM)

# General settings
experiment_name: alphazero_gomoku_15x15
checkpoint_dir: checkpoints/alphazero_gomoku_15x15
log_dir: logs/alphazero_gomoku_15x15
tensorboard_dir: tensorboard/alphazero_gomoku_15x15

# Game settings
game_type: gomoku
board_size: 15
input_channels: 19  # With attack/defense planes

# Training pipeline settings
pipeline:
  # Number of iterations (self-play -> train -> evaluate cycle)
  num_iterations: 1000
  
  # Self-play phase
  games_per_iteration: 100
  parallel_self_play_workers: 4  # Fewer workers with larger batches each
  
  # Training phase
  training_window_size: 50000  # Number of recent games to keep
  checkpoint_interval: 5  # Save model every N iterations
  resume_from_checkpoint: true  # Resume from latest checkpoint if available
  
  # Evaluation phase
  evaluation_games: 100
  evaluation_threshold: 0.55  # Win rate needed to update best model

# MCTS settings for self-play
mcts:
  num_simulations: 400  # Increased to amortize batch collection overhead
  num_threads: 6  # 6 threads per engine (4 parallel games * 6 = 24 threads total)
  exploration_constant: 3.0
  
  # Temperature schedule
  temperature_moves: 15  # Reduced from 30 for faster convergence
  temperature_start: 1.0
  temperature_end: 0.1
  
  # Noise for exploration
  dirichlet_alpha: 0.3
  dirichlet_epsilon: 0.25
  
  # Virtual loss for parallel MCTS
  virtual_loss: 3.0
  
  # Batch settings - Optimized for low latency with good GPU utilization
  batch_size: 128  # Balanced batch size for <100ms moves
  batch_timeout_ms: 5  # Short timeout for responsiveness
  min_batch_size: 32  # Lower minimum to avoid stalls
  max_pending_evaluations: 256  # Moderate pending evals
  
  # Parallel leaf collection
  parallel_collection_threads: 12  # More threads for faster collection
  collection_batch_size: 16  # Smaller batches for lower latency
  
  # Memory features
  use_transposition_table: true
  transposition_table_size_mb: 64  # Smaller table to reduce overhead
  
  # Progressive widening
  use_progressive_widening: false  # Disable for faster node selection
  progressive_widening_c: 0.5  # Reduced from 1.0 for tighter exploration
  progressive_widening_k: 5.0  # Reduced from 10.0 for faster node selection

# Neural network architecture
neural_network:
  # Model selection
  network_type: resnet  # or "ddw_randwire"
  
  # DDW-RandWire settings
  ddw_channels: 128  # Reduced for faster inference
  ddw_num_blocks: 6  # Reduced for faster inference
  ddw_num_nodes: 16  # Reduced for faster inference
  ddw_graph_method: watts_strogatz
  ddw_ws_p: 0.75
  ddw_ws_k: 4  # Reduced connectivity
  ddw_dynamic_routing: false  # Disabled for speed
  ddw_seed: 42
  
  # ResNet settings (alternative)
  num_filters: 128  # Reduced from 256 for 2x faster inference
  num_res_blocks: 10  # Reduced from 20 for 2x faster inference
  
  # Common settings
  value_head_hidden_size: 256  # Reduced from 512
  use_batch_norm: true
  dropout_rate: 0.1  # Reduced for faster inference

# Training hyperparameters
training:
  # Batch settings
  batch_size: 2048  # Maximize GPU utilization for RTX 3060 Ti
  accumulation_steps: 1  # No gradient accumulation needed with large batch
  
  # Optimization
  optimizer: adam
  learning_rate: 0.001  # Reduced for larger batches
  lr_schedule: cosine
  lr_warmup_steps: 2000  # Increased for larger batches
  lr_min: 0.00001
  
  # Regularization
  weight_decay: 0.0001
  gradient_clip: 1.0
  
  # Loss weights
  policy_loss_weight: 1.0
  value_loss_weight: 1.0
  
  # Training duration per iteration
  epochs_per_iteration: 10
  max_steps_per_iteration: 5000
  early_stopping_patience: 3
  
  # Data augmentation
  use_augmentation: true
  augmentation_types:
    - rotation
    - reflection
  
  # Mixed precision training
  use_amp: true
  
  # Checkpointing
  save_optimizer_state: true
  keep_checkpoint_max: 10

# Self-play settings
self_play:
  # Game generation
  max_game_length: 0
  resignation_threshold: -0.98  # More aggressive resignation
  resignation_move_threshold: 15  # Earlier resignation
  
  # Parallel settings
  games_per_worker: 5  # Fewer games per worker for faster GPU turnover
  
  # Memory management
  clear_pools_every_n_games: 20  # Less frequent cleanup to reduce overhead
  force_gpu_cleanup_every_n_games: 50  # Less aggressive GPU cleanup
  
  # Data format
  save_format: npz  # numpy compressed format
  compression_level: 6

# Evaluation settings
evaluation:
  # Match settings
  num_parallel_games: 20  # More parallel games for GPU utilization
  games_per_match: 5
  
  # MCTS settings for evaluation (typically stronger)
  mcts_simulations: 1600
  temperature: 0.1  # More deterministic play
  
  # Comparison settings
  compare_with_previous_n: 3  # Compare with last N versions
  elo_k_factor: 32

# Memory management
memory:
  # System memory (64GB total)
  warning_threshold_gb: 32.0
  critical_threshold_gb: 40.0
  emergency_threshold_gb: 48.0
  
  # Pool settings
  node_pool_initial_size: 200000  # Larger initial pool for efficiency
  node_pool_max_size: 2000000  # Much larger max with 64GB RAM
  game_state_pool_size: 10000  # More game states for parallel games
  tensor_pool_size: 4096  # Larger tensor pool for big batches
  
  # GPU memory (8GB total)
  gpu_memory_fraction: 0.95  # Use almost all VRAM
  gpu_pool_initial_mb: 4096  # Half of VRAM
  gpu_pool_max_mb: 7168  # Leave ~1GB for system
  empty_cuda_cache_on_pressure: true
  
  # Cleanup settings
  check_interval_ms: 1000  # Increased to reduce overhead
  cleanup_interval_ms: 10000  # Increased to reduce overhead

# Resource limits
resources:
  # CPU settings
  max_cpu_percent: 95  # Use most CPU for tree search
  worker_cpu_affinity: true
  
  # GPU settings
  gpu_utilization_target: 95  # Target high GPU usage
  
  # Disk I/O
  max_disk_usage_gb: 100
  cleanup_old_games: true
  games_retention_days: 30

# Monitoring and debugging
monitoring:
  # Logging
  log_level: info
  log_to_file: true
  log_rotation_mb: 100
  
  # Metrics
  enable_tensorboard: true
  tensorboard_update_freq: 100
  
  # Performance tracking
  profile_enabled: false
  profile_warmup_steps: 100
  profile_active_steps: 200
  
  # Debugging
  save_debug_games: true
  debug_game_interval: 100
  enable_memory_tracking: true
  
  # Notifications
  enable_notifications: false
  notification_webhook: ""

# Distributed training settings (optional)
distributed:
  enabled: false
  backend: nccl
  master_addr: localhost
  master_port: 29500
  world_size: 1
  rank: 0