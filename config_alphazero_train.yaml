# Complete AlphaZero Training Pipeline Configuration
# Optimized for Ryzen 9 5900X (24 threads) + RTX 3060 Ti (8GB VRAM)

# General settings
experiment_name: alphazero_gomoku_9x9
checkpoint_dir: checkpoints/alphazero_gomoku
log_dir: logs/alphazero_gomoku
tensorboard_dir: tensorboard/alphazero_gomoku

# Game settings
game_type: gomoku
board_size: 15
input_channels: 19  # With attack/defense planes

# Training pipeline settings
pipeline:
  # Number of iterations (self-play -> train -> evaluate cycle)
  num_iterations: 1000
  
  # Self-play phase
  games_per_iteration: 100
  parallel_self_play_workers: 8
  
  # Training phase
  training_window_size: 500000  # Number of recent games to keep
  checkpoint_interval: 5  # Save model every N iterations
  
  # Evaluation phase
  evaluation_games: 100
  evaluation_threshold: 0.55  # Win rate needed to update best model

# MCTS settings for self-play
mcts:
  num_simulations: 400
  num_threads: 12
  exploration_constant: 1.25
  
  # Temperature schedule
  temperature_moves: 30
  temperature_start: 1.0
  temperature_end: 0.1
  
  # Noise for exploration
  dirichlet_alpha: 0.3
  dirichlet_epsilon: 0.25
  
  # Virtual loss for parallel MCTS
  virtual_loss: 3.0
  
  # Batch settings
  batch_size: 256
  batch_timeout_ms: 5
  
  # Memory features
  use_transposition_table: true
  transposition_table_size_mb: 128
  
  # Progressive widening
  use_progressive_widening: true
  progressive_widening_c: 1.0
  progressive_widening_k: 10.0

# Neural network architecture
neural_network:
  # Model selection
  network_type: ddw_randwire  # or "resnet"
  
  # DDW-RandWire settings
  ddw_channels: 64
  ddw_num_blocks: 6
  ddw_num_nodes: 8
  ddw_graph_method: watts_strogatz
  ddw_ws_p: 0.75
  ddw_ws_k: 4
  ddw_dynamic_routing: true
  ddw_seed: 42
  
  # ResNet settings (alternative)
  num_filters: 128
  num_res_blocks: 10
  
  # Common settings
  value_head_hidden_size: 256
  use_batch_norm: true
  dropout_rate: 0.3

# Training hyperparameters
training:
  # Batch settings
  batch_size: 512
  accumulation_steps: 4  # Gradient accumulation for effective batch size of 2048
  
  # Optimization
  optimizer: adam
  learning_rate: 0.002
  lr_schedule: cosine
  lr_warmup_steps: 1000
  lr_min: 0.00001
  
  # Regularization
  weight_decay: 0.0001
  gradient_clip: 1.0
  
  # Loss weights
  policy_loss_weight: 1.0
  value_loss_weight: 1.0
  
  # Training duration per iteration
  epochs_per_iteration: 10
  max_steps_per_iteration: 5000
  early_stopping_patience: 3
  
  # Data augmentation
  use_augmentation: true
  augmentation_types:
    - rotation
    - reflection
  
  # Mixed precision training
  use_amp: true
  
  # Checkpointing
  save_optimizer_state: true
  keep_checkpoint_max: 10

# Self-play settings
self_play:
  # Game generation
  max_game_length: 0
  resignation_threshold: -0.95
  resignation_move_threshold: 20
  
  # Parallel settings
  games_per_worker: 125  # 1000 total / 8 workers
  
  # Memory management
  clear_pools_every_n_games: 50
  force_gpu_cleanup_every_n_games: 100
  
  # Data format
  save_format: npz  # numpy compressed format
  compression_level: 6

# Evaluation settings
evaluation:
  # Match settings
  num_parallel_games: 20
  games_per_match: 5
  
  # MCTS settings for evaluation (typically stronger)
  mcts_simulations: 1600
  temperature: 0.1  # More deterministic play
  
  # Comparison settings
  compare_with_previous_n: 3  # Compare with last N versions
  elo_k_factor: 32

# Memory management
memory:
  # System memory (64GB total)
  warning_threshold_gb: 32.0
  critical_threshold_gb: 40.0
  emergency_threshold_gb: 48.0
  
  # Pool settings
  node_pool_initial_size: 50000
  node_pool_max_size: 500000
  game_state_pool_size: 5000
  tensor_pool_size: 2048
  
  # GPU memory (8GB total)
  gpu_memory_fraction: 0.8
  gpu_pool_initial_mb: 2048
  gpu_pool_max_mb: 6144
  empty_cuda_cache_on_pressure: true
  
  # Cleanup settings
  check_interval_ms: 500
  cleanup_interval_ms: 5000

# Resource limits
resources:
  # CPU settings
  max_cpu_percent: 80
  worker_cpu_affinity: true
  
  # GPU settings
  gpu_utilization_target: 85
  
  # Disk I/O
  max_disk_usage_gb: 100
  cleanup_old_games: true
  games_retention_days: 30

# Monitoring and debugging
monitoring:
  # Logging
  log_level: info
  log_to_file: true
  log_rotation_mb: 100
  
  # Metrics
  enable_tensorboard: true
  tensorboard_update_freq: 100
  
  # Performance tracking
  profile_enabled: false
  profile_warmup_steps: 100
  profile_active_steps: 200
  
  # Debugging
  save_debug_games: true
  debug_game_interval: 100
  enable_memory_tracking: true
  
  # Notifications
  enable_notifications: false
  notification_webhook: ""

# Distributed training settings (optional)
distributed:
  enabled: false
  backend: nccl
  master_addr: localhost
  master_port: 29500
  world_size: 1
  rank: 0