# AlphaZero Pipeline Configuration (Debug Version)

# Game settings
game_type: gomoku
board_size: 15
policy_size: 225 # Explicitly set to board_size^2

# Directory settings
model_dir: models
data_dir: data
log_dir: logs

# Neural network settings
network_type: resnet
use_gpu: true
num_iterations: 10
num_res_blocks: 12  # Increased for 8GB VRAM - original was 19
num_filters: 128  # Increased for 8GB VRAM - original was 256
input_channels: 17  # Match Gomoku's enhanced tensor representation (17 planes)

# Self-play settings
self_play_num_games: 1 # Reduce to 1 for testing
self_play_num_parallel_games: 1
self_play_max_moves: 20 # Limit max moves to prevent excessive runtime
self_play_temperature_threshold: 30
self_play_high_temperature: 1.0
self_play_low_temperature: 0.1

# MCTS settings
mcts_num_simulations: 8  # Keep simulations low
mcts_num_threads: 4  # Reduced threads for better debugging
mcts_batch_size: 4  # Reduced batch size
mcts_batch_timeout_ms: 50  # Shorter timeout to avoid throttling
mcts_exploration_constant: 1.5
mcts_temperature: 1.0
mcts_add_dirichlet_noise: true
mcts_dirichlet_alpha: 0.3
mcts_dirichlet_epsilon: 0.25
mcts_virtual_loss: 3

# Training settings
train_epochs: 10
train_batch_size: 256
train_num_workers: 4
train_learning_rate: 0.001
train_weight_decay: 0.0001
train_lr_step_size: 5
train_lr_gamma: 0.1

# Arena/evaluation settings
enable_evaluation: true
arena_num_games: 20
arena_num_parallel_games: 4
arena_num_threads: 4
arena_num_simulations: 100
arena_temperature: 0.1
arena_win_rate_threshold: 0.55