# DDW-RandWire-ResNet Configuration for RTX 3060 Ti (8GB VRAM)
# Optimized for Ryzen 9 5900X (12 cores / 24 threads) + 64GB RAM
# Based on optimized ResNet configuration

# Game settings
game_type: gomoku
board_size: 9
input_channels: 17  # Gomoku enhanced representation with history

# Network type selection
network_type: ddw_randwire  # Use DDW-RandWire-ResNet instead of standard ResNet

# Model path
model_path: models/ddw_randwire_model.pt

# DDW-RandWire-ResNet specific settings  
ddw_channels: 64          # Reduced from 128 to match ResNet optimization
ddw_num_blocks: 10        # Reduced from 15 to match ResNet blocks
ddw_num_nodes: 16         # Reduced for better memory efficiency
ddw_graph_method: watts_strogatz  # Best performing method
ddw_ws_p: 0.75           # Rewiring probability
ddw_ws_k: 4              # Initial neighbors
ddw_er_edge_prob: 0.1    # For Erdos-Renyi (if used)
ddw_ba_m: 5              # For Barabasi-Albert (if used)
ddw_dynamic_routing: true # Enable instance-aware routing
ddw_seed: 42             # Fixed seed for reproducibility

# MCTS settings optimized for RTX 3060 Ti (from fixed config)
mcts_simulations: 600     # Reduced from 800 to reduce CPU load
mcts_threads_per_engine: 8   # Reduced from 12 to prevent CPU bottleneck
mcts_batch_size: 192      # Reduced from 256 for better CPU-GPU balance
mcts_batch_timeout_ms: 5  # Increased from 2 for better batching
mcts_c_puct: 1.0         # Reduced from 1.4 to match optimized
mcts_virtual_loss: 3.0
mcts_enable_transposition: true
mcts_temperature: 1.0
mcts_temp_threshold: 10
mcts_dirichlet_alpha: 0.25      # Added from optimized config
mcts_dirichlet_epsilon: 0.25    # Added from optimized config
mcts_use_progressive_widening: true    # Added for memory efficiency
mcts_progressive_widening_c: 1.0       # Added
mcts_progressive_widening_k: 10.0      # Added
mcts_transposition_table_size_mb: 64   # Reduced for memory control

# Parallel self-play settings for Ryzen 9 5900X
num_parallel_workers: 6   # Reduced from 8 to prevent CPU contention
num_games: 100
save_interval: 5
output_dir: data/self_play_games_ddw
max_game_length: 200      # Added from optimized config
clear_pools_every_n_games: 10      # Added for memory safety
force_gpu_cleanup_every_n_games: 20 # Added for memory safety

# Memory optimization settings for 64GB RAM system
# Allow much higher memory usage before triggering cleanup
max_memory_gb: 48        # Use up to 48GB (75% of 64GB)
cleanup_threshold_gb: 32  # Warning at 32GB (50% of 64GB)
emergency_cleanup_gb: 40  # Critical at 40GB (62.5% of 64GB)
enable_memory_pool: true  # Enable tensor pooling
tensor_pool_size: 1024    # Limit tensor pool
node_pool_initial_size: 10000     # Initial node pool
node_pool_max_size: 100000        # Max node pool
game_state_pool_size: 1000        # Game state pool
check_interval_ms: 200            # Memory check interval
cleanup_interval_ms: 2000         # Cleanup interval

# GPU-specific optimizations
cuda_device: 0
gpu_memory_fraction: 0.95  # Use 95% of VRAM
enable_mixed_precision: false  # Keep false for stability

# Advanced DDW settings (optional)
ddw_routing_temperature: 1.0    # Temperature for dynamic routing
ddw_edge_dropout: 0.0          # No dropout during inference
ddw_progressive_depth: false    # Use all blocks

# Performance monitoring
enable_profiling: false
log_interval: 100
monitor_gpu_usage: true

# Training settings (if using this config for training)
train_batch_size: 64      # Good for 8GB VRAM with DDW
train_learning_rate: 0.001
train_epochs: 20
train_num_workers: 4      # Data loading workers

# Notes on optimization:
# - DDW-RandWire-ResNet uses memory more efficiently than standard ResNet
# - Dynamic routing adds computational overhead but improves quality
# - Watts-Strogatz graphs typically perform best for game AI
# - With 8 workers and batch size 96, you'll get ~768 positions per GPU batch
# - This config should achieve 80-90% GPU utilization on your 3060 Ti