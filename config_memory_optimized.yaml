# AlphaZero Pipeline Configuration - Memory Optimized

# Game settings
game_type: gomoku
board_size: 15
policy_size: 0

# Directory settings
model_dir: models
data_dir: data
log_dir: logs

# Neural network settings
network_type: resnet
use_gpu: true
num_iterations: 5  # Reduced iterations
num_res_blocks: 12  # Keep same
num_filters: 128  # Keep same
input_channels: 17  # Match Gomoku's enhanced tensor representation

# Self-play settings
self_play_num_games: 10  # Reduced for testing
self_play_num_parallel_games: 2  # Reduced to limit memory
self_play_max_moves: 0
self_play_temperature_threshold: 30
self_play_high_temperature: 1.0
self_play_low_temperature: 0.1

# MCTS settings - CRITICAL FIXES
mcts_num_simulations: 100  # Reduced to prevent excessive memory usage
mcts_num_threads: 4  # Reduced thread count
mcts_batch_size: 16  # Smaller batch size for quicker processing
mcts_transposition_table_size_mb: 8  # Much smaller transposition table
mcts_use_transposition_table: false  # Disable to avoid memory issues
mcts_batch_timeout_ms: 5  # Reduced timeout for quicker batching
mcts_exploration_constant: 1.5
mcts_temperature: 1.0
mcts_add_dirichlet_noise: true
mcts_dirichlet_alpha: 0.3
mcts_dirichlet_epsilon: 0.25
mcts_virtual_loss: 3  # Explicitly set virtual loss

# Training settings
train_epochs: 5  # Reduced
train_batch_size: 64  # Reduced
train_num_workers: 2  # Reduced
train_learning_rate: 0.001
train_weight_decay: 0.0001
train_lr_step_size: 5
train_lr_gamma: 0.1

# Arena/evaluation settings
enable_evaluation: false  # Disable evaluation for now
arena_num_games: 10
arena_num_parallel_games: 2
arena_num_threads: 2
arena_num_simulations: 50
arena_temperature: 0.1
arena_win_rate_threshold: 0.55
EOF < /dev/null
